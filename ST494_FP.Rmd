---
title: "Predicting GPA and Interpreting Results"
author: "Brandon Chan, Lucas Duncan, Owen Macgowan"
date: "2025-03-07"
output: 
  html_document: 
    df_print: kable 
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(tibble.print_min = 4, tibble.print_max = 10) 

if(! require("caret")){install.packages("caret")}
if(! require("tidyverse")){install.packages("tidyverse")}
if(! require("ISLR2")){install.packages("ISLR2")}
if(! require("boot")){install.packages("boot")}
if(! require("MASS") ){ install.packages("MASS") }
if(! require("leaps") ){ install.packages("leaps") }
if(! require("glmnet") ){ install.packages("glmnet") }
if(! require("pls") ){ install.packages("pls") }
if(! require("splines")) {install.packages("splines")}
if(! require("e1071")){ install.packages("e1071")}
if(! require("pROC")){ install.packages("pROC")}
if(! require("class")){ install.packages("class")}
if(! require("reshape2")){ install.packages("reshape2")}
if(! require("DAAG")){ install.packages("DAAG")}
if(! require("ROCR")){ install.packages("ROCR")}
if(! require("tree")){ install.packages("tree")}
if(! require("rpart")){ install.packages("rpart")}
if(! require("rpart.plot")){ install.packages("rpart.plot")}
if(! require("randomForest")){ install.packages("randomForest")}
if(! require("gam")){ install.packages("gam")}
if(! require("pls")){ install.packages("pls")}
if(! require("car")){ install.packages("car")}

# MOVING IMPORTS WITH THE REST

# if(!require("keras")) {install.packages("keras")}
# reticulate::install_python(version = '3.11.9')
# install_keras(method = "virtualenv")

# Note: keras must be loaded after python and tensorflow are installed
# Otherwise R session must be refreshed
library(keras)


library(tidyverse)
library(e1071)
library(MASS)
library(tidyverse)
library(caret)
library(leaps)
library(tidyr)
library(ggplot2)
library(cluster)
library(glmnet)
library(ROCR)
library(pROC)
library(randomForest)
library(tree)
library(gam)
library(pander)
library(knitr)
library(nnet)
library(rpart)
library(e1071)
library(pls)
library(car)

select <- dplyr::select


set.seed(10)
```

# Executive Summary and Contents

```{r}
contents <- tribble(
  ~Index, ~Category, ~Method,
  1, "Introduction and Tidying", "Importing, Wrangling and Encoding",
  2, "Unsupervised Learning and Analysis", "PCA and Clustering",
  3, "Preprocessing", "Variable Selection",
  4, "Supervised Regression", "Linear and Non-Linear Regression",
  5, "Supervised Linear Classification", "LDA and Logistic Regression",
  6, "Supervised Non-Linear Classification", "Tree-Based Methods and Support Vector Machines",
  7, "Deep Learning", "Neural Network",
  8, "Conclusion and Future Research", "Written Description", 
) 
contents
```

# 1. Introduction and Preprocessing

By analyzing different factors that contribute to student performance, we aim to provide insight into how institutions can improve educational and environmental factors that foster student success. This is an issue that we can relate to directly. As students ourselves, we understand that success in academics is influenced by several factors that include preparation and ability, but go beyond to include factors outside of school.

```{r, warning=FALSE, include=FALSE}
# 2.1 Extract Data
location = "https://raw.githubusercontent.com/BrandonYChan/Statistical-Learning/main/Student_performance_10k.csv"

df <- read_csv(location, show_col_types = FALSE)
```


```{r, include=FALSE}
# 2.2 Clean Data

df2 <- df |>
       rename(
         "has_subsidized_lunch" = "lunch",
         "has_prepared" = "test_preparation_course",
         "is_male" = "gender",
         "parent_education" = "parental_level_of_education",
       ) |> 
       mutate(
         is_male = as.integer(if_else(is_male =="male",1,0)),
         race_ethnicity = as.factor(str_sub(df$race_ethnicity, -1, -1)),
         parent_education = as.factor(parent_education),
         has_prepared = as.integer(has_prepared),
         gpa_letter = as.factor(if_else(gpa_letter == "Fail", "F", gpa_letter)),    
         
         gpa_desirable = as.factor(if_else(gpa_letter %in% c("Fail","D","C"), TRUE, FALSE)) 
       ) |>
       select(-c("roll_no"))   

# Set reference column: need to drop later to reduce multicollinearity
df2$race_ethnicity <- relevel(df2$race_ethnicity, ref="E")

df2 |> head(2)
```

## 1.1 The Student Performance Dataset

The dataset consists of attributes that may have an impact on performance in school. There are academic variables that include grades in different subjects and other variables like ethnicity, gender, and economic status of the student families. There are options for regression and classification problems with this data. With the given variables, GPA seems like the most likely response variable to represent student performance. After some wrangling, gpa_letter, gpa, and gpa_desirable represent different forms of the possible response variable to predict.   

We start by cleaning our data; renaming columns for clarity, converting column types, generating attributes, and filtering rows and columns. We filter duplicate rows, and interpolate using column means for missing numerical variables, we fill variables where possible to fix inconsistencies between gpa and gpa_letter, then drop the remaining rows missing key factor variables.

```{r}
# 2.3 Interpolate NA values

#Remove Any Duplicates
df3 <- distinct(df2)

#Sometimes GPA_Letter is missing --> Fill Using GPA
df3 <- df3 |>
        mutate(
          gpa_letter = case_when(
          !is.na(gpa_letter) ~ gpa_letter,
          gpa >= 80 ~ "A",
          gpa >= 62.5 ~ "B",
          gpa >= 50 ~ "C",
          gpa >= 37.5 ~ "D",
          gpa < 37.5 ~ "F",
          TRUE ~ sample(c("A","B","C","D","F"),1)
        ))

#Sometimes GPA is missing --> Estimate Using Letter
df3 <- df3 |>
        mutate(
          gpa = case_when(
          !is.na(gpa) ~ gpa,
          gpa_letter == "A" ~ sample(80:100,1),
          gpa_letter == "B" ~ sample(63:79, 1),
          gpa_letter == "C" ~ sample(50:62,1),
          gpa_letter == "D" ~ sample(38:49,1),
          gpa_letter == "F" ~ sample(0:37,1),
          TRUE ~ sample(0:100,1)
        ))

#Replace Missing NUMERIC Values With Column Mean
if (any(is.na(df3))) {
  df3[] <- lapply(df3, function(x) {
    if (is.numeric(x)) {
      replace(x, is.na(x), round(mean(x, na.rm = TRUE)))
    } else {
      x
    }
  })
}

#Dropping Missing Factor Values
df3 <- df3[complete.cases(df3[c("race_ethnicity", "parent_education", "gpa_desirable", "gpa_letter")]), ]   

head(df3, 2)
```


## 1.2 Variable Distributions

We now want to get more familiar with our data, looking first at the numeric columns, we observe all seem approximately normal, which is to be expected given their nature. We note they all seem to be within acceptable range (0,100). We next analyze the boolean columns, none of which have one boolean option which dominated the data. Notably there seems to be an approximately equal number of males and females, which is a good sign that our sample is proportional to an entire group. Finally we analyze the true factors, education and race, and once again no one factor dominates the data, while factors are not exactly normally distributed, nor should they be, there is healthy variation between rows with no one factor representing too few rows.

```{r}
## 2.4 Display Variable Distributions

#Display Numeric
df_numeric <- df3 |> select(any_of(c("gpa", "recent_math_score", "recent_science_score", "recent_writing_score"))) |> pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
ggplot(df_numeric, aes(x = value)) + geom_histogram(bins = 30) +  facet_wrap(~ variable, scales = "free")  

#Display Factor
df_factor <- df3 |> mutate(across(c("has_prepared", "has_subsidized_lunch", "is_male"), as.factor )) |> select(where(is.factor)) |>
                 pivot_longer(cols = everything(), names_to = "variable", values_to = "value") 
ggplot(df_factor, aes(x = value)) + geom_bar() + facet_wrap(~ variable, scales = "free") 
```

## 1.3 Encoded Data Frame

From here, our next step in processing is to convert our factor variables into one hot notation to be used in predictive models. We modify both parent_education and race, as well as converting boolean factor variables into integers for use in models.

```{r}
# 2.5 Format For Classification + Regression

df4 <- df3 |> 
  bind_cols(as.data.frame(model.matrix(~ `race_ethnicity` + `parent_education` - 1, data = df3))) |>
  select(-c("race_ethnicity","parent_education")) |>
  transform(has_subsidized_lunch = as.integer(has_subsidized_lunch), has_prepared=as.integer(has_prepared), is_male=as.integer(is_male))

head(df4, 2)
```

## 1.4 Handling Multi-Collinearity

To deal with the multicollinearity problem, we had to remove the reference column for the race_ethnicity variable to get appropriate VIF scores.

```{r}
vif_scores1 = vif(lm(gpa ~ ., data=df4 |> select(-c("gpa_letter", "gpa_desirable", "race_ethnicityn"))))

vif_scores2 = vif(lm(gpa ~ ., data=df4 |> select(-c("gpa_letter", "gpa_desirable", "race_ethnicityn", "race_ethnicityE"))))

data.frame("VIF Original" = head(vif_scores1, 10), "VIF with Dropped Ref." = head(vif_scores2, 10))
```


```{r. include=FALSE}
df4<- df4 |> select(-c("race_ethnicityn", "race_ethnicityE"))
```


```{r, include=FALSE}
# 2.6 Split Into Train + Test Sets

train_index <- sample(1:nrow(df4), nrow(df4)*0.8)

df_train <- df4[train_index,]
df_test <- df4[-train_index,]

pca_train <- df_train |> select(-c(gpa, gpa_letter, gpa_desirable))
pca_test <- df_test |> select(-c(gpa, gpa_letter, gpa_desirable))

```

# 2: Data Exploration with Unsupervised Methods (PCA and Clustering)

Before attempting to make any conclusive analysis, we want to get a better understanding of the data and relationships between variables. We start this process with unsupervised learning to understand variables that cause significant variance and groupings of observations. 

## 2.1 PCA: Generating and Visualizing Principle Components

We begin exploration by generating the principal components to create a 2-dimensional visualization. The first 2 principal components represent uncorrelated linear combinations that capture the most variance in the data. We plot them to see if any patterns emerge.

```{r, warning=FALSE, message=FALSE}
# Find Principal Components 
pc <- prcomp(df_train |> select(-c(gpa, "gpa_desirable","gpa_letter")), scale=TRUE) # Scale parameter brings features to the same scale with z-score standardization

pc_test <- prcomp(df_test |> select(-c(gpa, "gpa_desirable","gpa_letter")), scale=TRUE)
```


```{r, warning=FALSE, message=FALSE}
#Highlight Distribution Of TRAIN GPA in PC
gpa_colors <- as.factor(df_train$gpa_letter)
plot(pc$x, col = gpa_colors, pch=20, cex=2, main = "Clear Groupings Along Both Principal Component Axes")
legend("topright", legend = levels(gpa_colors), 
       fill = 1:length(levels(gpa_colors)), 
        title = "GPA Letter")

# biplot(pc, scale = 0, cex=0.4) # May find a use for this plot elsewhere
```

There appear to be 3 distinct groupings in the data, separable by the second principal component. Based on the GPA letter coloring of points, lower values of the first principal component appear to relate to higher GPA letters and vice-versa. 

## 2.2 K-means Clustering: Ideal Number of Clusters

In order to check our theory on the distinct groupings of the first two principal components, we will employ k-means clustering. But first, we need to determine the optimal number of clusters to effectively show potential classes. 

```{r, warning=FALSE}
# within_cluster_sum_squares <- c()
# 
# # Calculate wcss for each number of clusters
# for(k in 1:10){
#   clusters <- kmeans(pc$x[,1:2], centers = k, nstart=100)
#   within_cluster_sum_squares[k] = clusters$tot_withinss
# }
# 
# plot(x=1:10, y=within_cluster_sum_squares)

# Elbow method did not work 

# Silhoutte Method

pc_1_2 <- pc$x[,1:2] # First 2 principal components
avg_scores = c()

# Find silhouette scores for 2:10 clusters 
for(k in 2:10){
  clusters = kmeans(pc_1_2, centers=k, nstart=25) 
  scores <- silhouette(clusters$cluster, dist(pc_1_2))
  avg_scores[k-1] = mean(scores[,3])
}

max_k <- which.max(avg_scores) + 1

# Choose where there is a peak on the graph 
plot(x=2:10, y=avg_scores, type="b", xlab="k", ylab="Silhouette Score", main=paste("Suggests", max_k,  "Clusters"))
points(x=max_k, y=avg_scores[max_k-1], col="red", cex=2, pch=20) # Peak at k=3

ideal_clusters <- max_k
```

A peak in the silhouette score appears where there are 4 different clusters, an ideal number where points are similar within a cluster and different from other clusters.

## 2.3 K-Means Clustering on First 2 Principal Components

We color the points on the same principal component graph, this time by the cluster that they belong to.

```{r}
km <- kmeans(pc$x[,1:2], centers=ideal_clusters, nstart=25) 
plot(pc$x, col = (km$cluster +1), pch=20, cex=2)
```

Clusters appear to be clearly separated by both principal components, indicating that our hypothesis that the first component significantly splits the data into groups and academic feature such as grades may impact the second principal component appear plausible. 

## 2.4 Impact of Variables on the First Principle Component

We wanted to take a closer look at specific features in addition to the overall structure of the data and principal components. So we took the top ten largest loadings (coefficients) of the first principal component. The goal is determine which variables are responsible for variance within the feature set.

```{r}
pc1 <- pc$rotation[,1]
pc1_loading_df <- cbind(names(pc1), tibble(pc1)) |>
                  rename("Variable Names" = "names(pc1)", "PC1 Loading" = "pc1") |> 
                  arrange(desc(abs(`PC1 Loading`)))
pc1_loading_df |> head(10)
```

Having a subsidized lunch and staying prepared appear to be the most significant features impacting variance in the data. Although a subsidized lunch has no obvious direct relationship to academics, it appears that socio-economic factors may be at play in some fashion. Taking an extra preparation course also seems to have a significant impact in the feature space. Preparation likely has a direct relationship with academic performance features.

## 2.5 Proportion of Variance Explained (PVE)

Only the first principal component was analyzed, and other principal components may tell very different stories about which features may be significant. We show how much each additional principal component contributes to the proportion of variance explained:

```{r, warning=FALSE}
pve <- pc$sdev^2 / sum(pc$sdev^2)   

pve_df <- as_data_frame(cbind(1:length(pve), pve)) |> # Show PVE for each number of principal components
  rename("PVE" = "pve") |> 
  mutate("Cumulative PVE" = cumsum(PVE)) |> # Cumulative PVE for each additional PC 
  rename("Number of Principal Components" = "V1")

pve_df |> head(10)
```

Additional principal components contribute significantly to higher PVE. The first principal component does not contribute to much of the variance explained and the second principal component does not increase it significantly either. We interpret this to mean that there are many notable linear combinations of features rather than a one that significantly explains variance the feature space.

## 2.6 Partial Least Squares Prediction for GPA

Our analysis on the principal components is predicated on the assumption that they having meaning in relation to evaluating academic performance. We evaluate this by using partial least squares to predict GPA.

```{r}
pls_reg_model <- plsr(gpa ~ ., ncomp=15, validation="CV", data=df_train |> select(-c(gpa_desirable, gpa_letter)))

validationplot(pls_reg_model, val.type="RMSEP")
```

3 components appears optimal based on the PVE and plot.

```{r}
pls_pred_regression <- predict(pls_reg_model, newdata=df_test |> select(-c(gpa_desirable, gpa_letter)), ncomp=3)

data.frame(RMSE = RMSE(pls_pred_regression, df_test$gpa)) 

```

We will compare the error with other methods later in the analysis to determine how effective the principal component are at predicting GPA, our measure for academic performance.

# 3: Variable Selection for Supervised Methods

Before proceeding to predict GPA with supervised linear methods, we want to ensure that predictor variables used are relevant. It could also be helpful to analyze which predictors contribute to academic success. This will help to determine what factors institutions and individuals can improve to aid in academics.

## 3.1 Evaluating Criterion for Best Subset Selection

Since the dataset is not too large (~10000 rows), we feel that using best-subset selection is computationally feasible and is the optimal method for variable selection. 3 types of evaluation criterion are employed to determine the ideal variables; AIC, BIC, and adjusted R-squared. 

```{r, warning=FALSE, message=FALSE}
par(mfrow=c(1,3))

# Note: Adjusted plots to start from 3 predictors; looks cleaner 

# Best subset selection
best_subset <- regsubsets(gpa~., df_train |> select(-c("gpa_letter", "gpa_desirable")), nvmax = ncol(df_train))
bs_summary <- summary(best_subset)

# Number of predictors in dataframe used 
n_preds_uncorr <- ncol(df_train)-3

# BIC plot
plot(3:n_preds_uncorr, bs_summary$bic[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="BIC")
points(which.min(bs_summary$bic), min(bs_summary$bic), col="red", cex=2, pch=20) 

# AIC plot
plot(3:n_preds_uncorr, bs_summary$cp[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="AIC")
points(which.min(bs_summary$cp), min(bs_summary$cp), col="red", cex=2, pch=20)

#SHOULD WE KEEP THIS?????? 
# lines(seq_along(bs_summary$cp), seq_along(bs_summary$cp)+1, col="gray", lty=2)

# Adjusted R^2 plot
plot(3:n_preds_uncorr, bs_summary$adjr2[3:n_preds_uncorr], type="b", xlab="Number of predictors", ylab="Adj. R^2")
points(which.max(bs_summary$adjr2), max(bs_summary$adjr2), col="red", cex=2, pch=20)
```

The best number of predictors appears different but fairly similar given each criterion. BIC heavily penalizes additional variables, so it is expected to favour a lower number of variables. AIC appears to prefer one more variable and few more variables are favoured with $R^2_{adj}$. 

## 3.2 Ideal Number of Variables for Each Criterion

```{r}
# 4.2 Extracting Key Variables

tibble(Best_BIC = which.min(bs_summary$bic), Best_Cp = which.min(bs_summary$cp), Best_Adjusted_Rsq = which.max(bs_summary$adjr2))
```

Since BIC heavily penalizes extra predictors and adjusted $R^2_{adj}$ appears to be more lenient, we choose to use 6 predictors, a value in the middle.

## 3.3 Names of Ideal Predictors

The following variables contribute the most according to best subset selection:

```{r}
p = 6  #currently arbitrarily chosen, better method???

chosen_model <- bs_summary$which[p, ]
chosen_predictors = gsub("`", "", names(chosen_model[chosen_model == TRUE]))
data.frame(ChosenPredictors=chosen_predictors[2:length(chosen_predictors)])
# We should make this output cleaner in the final edit 
```

We update train and test sets to only includes these predictors.

```{r, include=FALSE}
# 4.3 Filtering Data Columns

#UPDATING TRAIN + TEST TO ONLY INCLUDE IMPORTANT VARIABLES
selected_train <- df_train[, c("gpa","gpa_letter", "gpa_desirable", chosen_predictors[2:length(chosen_predictors)])]
selected_test <- df_test[, c("gpa","gpa_letter", "gpa_desirable",chosen_predictors[2:length(chosen_predictors)])]

colnames(selected_train) <- gsub(" ", "_", colnames(selected_train)) 
colnames(selected_train) <- gsub("'", "", colnames(selected_train))

colnames(selected_test) <- gsub(" ", "_", colnames(selected_test)) 
colnames(selected_test) <- gsub("'", "", colnames(selected_test))

colnames(selected_train)
```

# 4 Regression for Identifying Important Predictors of Academic Success

With the selected variables, we can now test how well they predict academic performance measured in GPA. We start by training supervised regression models to predict the GPA score.

## 4.1 Choosing the Most Effective Linear Method

The base linear regression model, as well as linear models with LASSO and Ridge regularization are created and compared. 

```{r}
train_x <- as.matrix(selected_train[, !colnames(selected_train) %in% c("gpa", "gpa_letter", "gpa_desirable")])  
train_y <- as.matrix(selected_train$gpa)

test_x <- as.matrix(selected_test[, !colnames(selected_test) %in% c("gpa", "gpa_letter", "gpa_desirable")])  
test_y <- as.matrix(selected_test$gpa)

#Linear Model
linear_model <- lm(gpa ~ ., data=selected_train[,!colnames(selected_train) %in% c("gpa_letter", "gpa_desirable")])  
linear_preds <- predict(linear_model, selected_test[,!colnames(selected_train) %in% c("gpa_letter", "gpa_desirable")])  
RMSE_linear = RMSE(test_y, linear_preds)
MAE_linear = MAE(test_y, linear_preds)

#LASSO:
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=1)$lambda.min
lasso_model <- glmnet(train_x, train_y, alpha=1, lambda=optimal_lambda)
lasso_predictions <- predict(lasso_model, newx=test_x, s=optimal_lambda)
RMSE_lasso =  RMSE(test_y, lasso_predictions)
MAE_lasso = MAE(test_y, lasso_predictions)

#Ridge
optimal_lambda <- cv.glmnet(train_x, train_y, alpha=0)$lambda.min
ridge_model <- glmnet(train_x, train_y, alpha=0, lambda=optimal_lambda)
ridge_predictions <- predict(ridge_model, newx=test_x, s=optimal_lambda)
RMSE_ridge = RMSE(test_y, ridge_predictions)
MAE_ridge = MAE(test_y, ridge_predictions)

tribble(~"Loss Type", ~"Unregularised Linear", ~"LASSO", ~"Ridge",
        "RMSE", RMSE_linear, RMSE_lasso, RMSE_ridge,
        "MAE", MAE_linear, MAE_lasso, MAE_ridge
        )

```

Interestingly, the unregularized model outperforms LASSO and Ridge regression. This may be because we performed regularization after variable selection, so irrelevant predictors were already filtered out. Additional penalties on predictors probably only interfered with making the best predictions that minimize SSE.

## 4.2 Extracting Insight And Evaluating Chosen Linear Method

Before making conclusive analysis, we want to ensure that the model is predicting accurately on test data. 

### Accuracy Within 10%

Since the linear regression model only predicts continuous values, it wouldn't make sense to calculate accuracy directly, but we still want to ensure that the model is accurate. We resolve this by predicting how frequently predictions are within 10 points of the true GPA.  

```{r}
accuracy_within_10 <- mean(abs(selected_test$gpa -lasso_predictions) < 10)
print(paste("Accuracy within 10 GPA points: ", accuracy_within_10))
```

The vast majority of predictions are within 10 points of the true GPA, so we feel that analysis on the impact of each attribute will be meaningful.

```{r}
#LASSO MODEL EVALUATION:
error_colors <- as.factor(abs(selected_test$gpa - linear_preds) < 10)
plot(pc_test$x, col = error_colors, pch=20, cex=2, main="Nearly All Predictions are Within 10% of True GPA")
legend("topright", legend = levels(error_colors), 
       fill = 1:length(levels(error_colors)), 
       pch = 20, title = "Reasonable Prediction")
```

Since the model is fairly accurate at predicting GPA, we feel that analyzing coefficients directly can provide insight into how each predictor contributes to success.

```{r}
coef(linear_model)
```

Of the encoded variables, gender appears to be the most significant based on the raw coefficient, followed by parent education and preparation. These values suggest that societal factors meaningfully impact academic success, potentially because of different educational opportunities. Pure academic scores appear to contribute similarly to each other. This may be because students successful in one subject tend to be successful overall.

## 4.3 Non-linear Regression: GAM

Although the linear model was successful, but there remains a possibility that prediction is more accurate with non-linear transformations applied to variables. We employ a generalized additive model with splines determined by cross-validation.

```{r, include=FALSE}
# Cross Validation to Choose the Best One

chosen_preds_tidy <- preds_formula <- gsub(" ", "_", chosen_predictors[-c(1)])
preds_formula <- paste(chosen_preds_tidy, collapse=" + ")

gam_obj <- gam(as.formula(paste("gpa~", preds_formula)), data=selected_train)


scope_list <- list()

for(i in seq_along(chosen_preds_tidy)){
  predictor_name <- chosen_preds_tidy[i]
  
  length_unique <- nrow(distinct(selected_train[chosen_preds_tidy[i]]))
  
  if(length_unique > 2){
    formula_curr <- as.formula(paste("~1 +", predictor_name, "+", "s(",predictor_name, ", 2)", "+ s(", predictor_name, ", 3)"))
  }
  else{
    formula_curr <- as.formula(paste("~1 +", predictor_name))
  }
  scope_list[[predictor_name]] <- formula_curr
}

scope_list

step.Gam(gam_obj, scope=scope_list)

# summary(gam_obj)
```

```{r}
best_gam <- gam(formula = gpa ~ is_male + s(recent_math_score, 3) + recent_writing_score + 
    recent_science_score + parent_educationsome_high_school, 
    data = selected_train, trace = FALSE)

preds_best_gam <- best_gam %>% 
  predict(selected_test)
```

## 4.4 Model Comparison

At this point we have the best supervised linear, non-linear, and unsupervised models. By comparing each of them, we hope to obtain the best one and to understand why it performed the best. Once the model is understood, we will attempt to relate the model to real-world academic performance.

```{r}
tribble(~Measurement, ~GAM, ~LinearModel, ~PLS,
        "RMSE", RMSE(preds_best_gam, selected_test$gpa),RMSE(linear_preds, selected_test$gpa), RMSE(pls_pred_regression, df_test$gpa),
        "R2", caret::R2(preds_best_gam, selected_test$gpa), caret::R2(linear_preds, selected_test$gpa), caret::R2(pls_pred_regression, df_test$gpa))
```

The generalized additive model performs the best in terms of RMSE. Non-linear splines applied to predictors appear to improve the model, indicating that GPA is not best determined by a linear combination of features. The PLS predictions formed by the first 3 principal components perform slightly worse, though not by a drastic margin. Reducing dimensionality or the number of variables seems to help the model in all methods. 

Based on the results of applying regression to predict GPA, societal and educational factors both contribute to academic success.


# 5: Linear Classification for Evaluating Predictive Accuracy of GPA Predictors

The regression models allowed us show importance within the model of specific variables by analyzing coefficients, but we were unable to compute easily interpretible metrics such as accuracy directly. The purpose of our analysis is to translate our findings to the real world, and we think that a classification perspective will provide more interpretable results. Our goal in this section is to determine whether a linear decision boundary can differentiate between students having a desirable GPA.

## 5.1 LDA Model

The first classification method we attempt is linear discriminant analysis. We train it using the same predictors determined by variable selection.

```{r, include=TRUE}
lda_model <- lda(gpa_desirable ~ ., data = selected_train |> select(-c("gpa", "gpa_letter")))

lda_preds <- predict(lda_model, newdata = selected_test |> select(-c("gpa", "gpa_letter")))

lda_class <- lda_preds$class
```

We plot the distribution of LD1 discriminant scores to look for skew or multimodal properties.

```{r, include=TRUE}
df_lda <- data.frame(lda_preds$x, gpa_class=selected_test$gpa_desirable)

df_lda |> 
  ggplot(aes(x=LD1), color=gpa_class) +
  geom_density() +
  labs(title="LD1 Disciminant Score Distribution")
```

There appears to be a multimodal peak representing each class although it is very subtle. The distribution also appears to have a slight skew right. We interpret these properties to mean that there is a noticeable distinction between classes and there are more extreme large discriminant scores. This could mean that some students are strongly predicted to have a good GPA given the predictors.

We evaluate the LDA model with a confusion matrix and related metrics.

### Confusion Matrix

```{r}
cm_lda <- confusionMatrix(lda_class, selected_test$gpa_desirable)

cm_lda_df <- as.data.frame(cm_lda$table) |> 
  rename(Label=Reference)

cm_lda_df |> 
  ggplot(aes(x=Label, y=Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label=Freq), color="white") +
  scale_fill_gradient(low="lightblue", high="darkblue") +
  labs(title="LDA Confusion Matrix for Desirable GPA")

```

### Metrics

```{r}
# accuracy_lda <- mean(lda_class == selected_test$gpa_desirable)
# print(paste("LDA accuracy: ", accuracy_lda))

tribble(~Metric, ~Score,
        "Accuracy", cm_lda$overall['Accuracy'],
        "Sensitivity", cm_lda$byClass['Sensitivity'],
        "Specificity",cm_lda$byClass['Specificity'],
)
```

The LDA model seem pretty strong based on the confusion matrix and metrics. It seems to predict the true class very well, but is not as good at predicting true negatives. This may be due to a class imbalance. 

```{r, include=FALSE}
#LDA MODEL EVALUATION:
error_colors <- as.factor(selected_test$gpa_desirable == lda_class)

plot(pc_test$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")


predicted_probs <- lda_preds$posterior
colnames(predicted_probs) <- levels(selected_test$gpa_desirable)
```

## 5.3 Logistic Model 

```{r, include=FALSE}
log_reg_model <- multinom(gpa_desirable ~ ., data = selected_train |> select(-c("gpa","gpa_letter")))
 
#log_reg_pred_probs <- predict(log_reg_model, newdata = test_cls, type = "probs")

log_reg_pred_class <- predict(log_reg_model, newdata = selected_test |> select(-c("gpa","gpa_letter")), type = "class")
 
 
log_reg_accuracy <- mean(log_reg_pred_class == selected_test$gpa_desirable)

```

### Confusion Matrix

```{r}
cm_lr <- confusionMatrix(log_reg_pred_class, selected_test$gpa_desirable)

cm_lr_df <- as.data.frame(cm_lr$table) |> 
  rename(Label=Reference)

cm_lr_df |> 
  ggplot(aes(x=Label, y=Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label=Freq), color="white") +
  scale_fill_gradient(low="lightblue", high="darkblue") +
  labs(title="Logistic Regression Confusion Matrix for Desirable GPA")
```

### Metrics

```{r}
tribble(~Metric, ~Score,
        "Accuracy", cm_lr$overall['Accuracy'],
        "Sensitivity", cm_lr$byClass['Sensitivity'],
        "Specificity",cm_lr$byClass['Specificity'],
)
```

The logistic regression model appears slightly superior to the LDA model at predicting the negative class, but results are fairly similar overall. 

### Plotting Logistic Regression Model Evaluation

```{r, include=FALSE}
 #LOGISTIC MODEL EVALUATION:
error_colors <- as.factor(log_reg_pred_class == selected_test$gpa_desirable)
plot(pc_test$x, col = error_colors, pch=20, cex=2)
legend("topright", legend = levels(error_colors),
       fill = 1:length(levels(error_colors)),
       pch = 20, title = "Reasonable Prediction")
```

Based on the high accuracy of the linear classifiers, a linear decision boundary can effectively differentiate between students with a good GPA and a poor GPA. This shows that the predictors are indicative of academic performance and if students are able to improve some of the attributes, they are likely to be more successful in school.     

# 6: Classification Analysis and Visualization with Tree-Based Methods and SVM 

We now know that the predictors can effectively distinguish between strong and weak academic performers, but other methods may be more accurate and provide more insight into how predictions are being made.

## 6.1 Decision Tree

The decision tree is particularly useful for our case because it is interpretable, and we want to show what predictive variables are causing students to perform well in school. 

```{r, include=FALSE}
train_input <- selected_train |> select(-gpa, -gpa_letter) 
decision_tree <- rpart(gpa_desirable ~ ., data=train_input, parms = list( split = "information"))

summary(decision_tree)
```

```{r}
rpart.plot(decision_tree, main="Tree for Classifying Strong Academic Performance")
```

The decison tree with splits based on information gain heavily favours academic scores for determining student performance. Among them, it considers math the most significant split with the highest information gain. Many areas of science rely on a strong math background, which could explain why math score is the first split.

## 6.2 Random Forest

We follow up the decision tree by training a random forest. Since we are attempting to extract inference rather than maximize predictive accuracy, we do not show evaluation metrics on test data. Instead, we plot variable importance to show which predictive variables strongly considered by the random forest model.

```{r, include=FALSE}
random_forest_model <- randomForest(gpa_desirable~., train_input, ntree=500, importance=TRUE)
```

```{r}
varImpPlot(random_forest_model, main="Random Forest Variable Importance")
```

The variable importance plot strengthens the hypothesis that decision trees and random forest heavily favour the academic predictor variables.

## 6.3 SVM Kernel Analysis 

We apply the kernel trick to train several support vector machines in order to analyze the ideal shape of the decision boundary. 

```{r, include=FALSE}
try = seq(1,1 #10
          ,by=1)

# #ATTEMPTING TO DETERMINE THE RELATIONSHIP OF DATA (IE LINEAR,QUADRATIC,RADIAL)
# 
# #from PC, we expect Linear Model To Work Well
# 
# 
train <- selected_train |> select(-c("gpa", "gpa_letter"))
test <- selected_test |> select(-c("gpa", "gpa_letter"))


selected_train
best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try),kernel="linear")$best.parameters$cost
 linear_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="linear")
 lin_preds <- predict(linear_kernel_svm, test, type="class") != test$gpa_desirable
 lin_mis <- mean(lin_preds)

 best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try), kernel="radial")$best.parameters$cost

 radial_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="radial")
 rad_mis <- mean(predict(radial_kernel_svm, test, type="class") != test$gpa_desirable)
  
 
  best <- tune(svm, gpa_desirable ~ ., data = train, ranges = list(cost = try), kernel="polynomial",degree=2)$best.parameters$cost
 quadratic_kernel_svm <- svm(gpa_desirable ~ .,data=train, cost=best, kernel="polynomial",degree=2)
 quad_mis <- mean(predict(quadratic_kernel_svm, test, type="class") != test$gpa_desirable)
#
```

```{r}
tribble(~"Kernel.Type", ~"Misclassification",
        "Linear", lin_mis,
        "Radial", rad_mis,
        "Quadratic", quad_mis)
```

The support vector machine with the radial kernel performs the best, followed closely by the linear kernel and distantly by the quadratic kernel. Since both linear and radial kernels perform well, two seemingly contradictory results emerge. The linear decision boundary suggests a simpler relationship between predictor variables to classify academic performers, while the strong performance of the SVM with the radial kernel suggests a more complex one. In the context of academic performance, this may mean that societal and academic predictive factors can be used in a straightforward manner to distinguish strong academic performers, but there are complex relationships between variables that are better separated with a complex decision boundary.     

```{r, include=FALSE}
 error_colors <- as.factor(!lin_preds)
 plot(pc_test$x, col = error_colors, pch=20, cex=2)
 legend("topright", legend = levels(error_colors), 
        fill = 1:length(levels(error_colors)), 
        pch = 20, title = "Reasonable Prediction")
```

## 6.4 Deep Learning for Analyzing Specific GPA Scores

From analyzing different classification methods and kernels, we determined that complex relationships exist between predictors of academic success. We also previously identified that strong academic performers can be accurately identified, but specificity tends to be far worse than sensitivity. This leads us to think that certain GPA scores are being misclassified consistently, while others are being classified well. We attempt to verify the claim by creating a deep learning model with softmax activation for multiclass classification output. 


```{r, include=FALSE}
X_nn_train <- as.matrix(selected_train |> select(-c("gpa_letter", gpa_desirable, gpa)))
Y_nn_train <- as.matrix(model.matrix(~gpa_letter -1, data=selected_train))

nn_keras <- keras_model_sequential() |>
  layer_dense(units=50, activation="relu", input_shape=6) |>
  layer_dropout(rate=0.1) |>
  layer_dense(units=25, activation="relu") |>
  layer_dense(units=length(unique(selected_train$gpa_letter)), activation="softmax")

nn_keras |>
  compile(
    loss="categorical_crossentropy",
    optimizer=optimizer_adam(),
    metrics=c("accuracy")
  )

nn_keras |>
  fit(
    X_nn_train, Y_nn_train,
    epochs=25,
    batch_size=10
  )
```

```{r, include=FALSE}
X_nn_test <- as.matrix(selected_test |> select(-c("gpa_letter", gpa_desirable, gpa)))
Y_nn_test <- as.matrix(model.matrix(~gpa_letter -1, data=selected_test))
# summary(nn_keras)
pred_nn_keras_prob <- predict(nn_keras, X_nn_test, type="class")
colnames(pred_nn_keras_prob) <- c("gpa_letterA", "gpa_letterB", "gpa_letterC", "gpa_letterD", "gpa_letterF")
pred_nn_keras <- colnames(pred_nn_keras_prob)[apply(pred_nn_keras_prob, 1, which.max)]
Y_nn_test_label <- colnames(Y_nn_test)[apply(Y_nn_test, 1, which.max)]

acc_nn_keras <- mean(pred_nn_keras == Y_nn_test_label)
data.frame(Accuracy=acc_nn_keras)
```

```{r}
letter_grades <- c("A", "B", "C", "D", "F")

for(letter in letter_grades){
  letter_label = paste("gpa_letter", letter, sep="")
  
  acc <- sum(pred_nn_keras == letter_label & Y_nn_test_label == letter_label)/sum(Y_nn_test_label == letter_label)
  
  acc <- round(acc *100, 2)
  
  print(paste(letter, " accuracy: ", acc, "%"))
}

```

It appears that the neural networks classifies decent grades (B) very well. The model predicts weak albeit passing grades fairly accurately, but performs much more poorly classifying very good or failing grades. We think this is largely a consequence of the predictors; they are able to identify students with performance at the extremes, either negative or positive. This suggests that students with an A or F GPA distinguish themselves from peers in the same circumstances as themselves. Their individuality sets them on a unique path to achieve either great academic success of failure.

# 8 - Conclusions and Future Research

## 8.1 Conclusions 
-   DOT JOT MEANT TO BE EXPANDED (just my initial ideas -LD)
-   previous individual subject marks = best indicator (which is best??)
-   impact of parent education (economic + social impact? perspectives on school)
-   lunch \~ income range (tends to have less access to resource, or luxury of time)

## 8.2 Future Research

OPTIONAL????????

-   DOT JOT MEANT TO BE EXPANDED (just my initial ideas -LD)
-   current emphasis on racial and economic factors + previous success
-   expansion of analysis possible for geographic, ie between schools with varying urban/rural divide,
-   possible opportunity to look at student mental health (IE: psycology, risk behaviours)
-   maybe even something about access to technology, a metric about technical capacity \~ assuming technical skills give indication for success in increasingly technical school, idk, that last bit is a stretch
